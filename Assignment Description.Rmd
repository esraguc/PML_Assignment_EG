---
title: "Assignment Description"
author: "Esra Guc"
date: "07/02/2021"
output:
  html_document: default
  pdf_document: default
---
 First I loaded libraries that I will need for this assignment
 
```{r}
library(caret)
library(lattice)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(corrplot)
library(randomForest)
library(rattle)
```
Then I set the seed and installed the data from my computer
```{r}
setwd("/Users/eguc/Desktop")
set.seed(123)
train_dat<- read.csv("pml-training.csv")[,-1]
test_dat<- read.csv("pml-testing.csv")[,-1]
dim(train_dat)
dim(test_dat)
```
There are 159 variables in this data set however some variables have a lot of NA, some has almost 0 variation 
also first 5 variables wont be used in training so I removed these
```{r}
Zerovar<- nearZeroVar(train_dat)
training<- train_dat[,-Zerovar]
testing<- test_dat[,-Zerovar]
NAVal <- sapply(training, function(x) mean(is.na(x))) > 0.95
training <- training[, NAVal == "FALSE"]
testing <- testing[, NAVal== "FALSE"]
training <- training[,-c(1:5)]
testing <- testing[,-c(1:5)]
```
I checked the number of variables again 
```{r}
dim(training)
dim(testing)
```
variable numbers are decreased to 53


Then I started preparing the training and testing data to test models. I will use Decision Tree  and Random Forest models. I created two partitions ( 70% and 30%) from training data. 
```{r}
inTraining<- createDataPartition(y=training$classe, p=0.7, list=FALSE)
train_final<- training[inTraining, ]
test_final<- training[-inTraining, ]
```
I first tested Decision Tree Modeling. I classified the data argument in as.factor so data and reference factors will be in the same number of levels. 
```{r}
mod_DT <- train(classe ~ .,data=train_final, method = "rpart")
predict_train_DT <- predict(mod_DT, train_final)
ConfM_train_DT <- confusionMatrix(predict_train_DT,as.factor(train_final$classe))
predict_test_DT <- predict(mod_DT, test_final)
ConfM_test_DT <- confusionMatrix(predict_test_DT,as.factor(test_final$classe))
print(ConfM_test_DT)
```
Decision Tree Modeling gives an 0.4997 accuracy. Then I tried Random Forest Modeling. However Random Forest data processing takes very long, I got this tip from Coursera Forum, data needs to be tuned using tuneGrid

```{r}
mod_RF <- train(classe ~ .,data=train_final, method = "rf", trControl=trainControl(method="none"), tuneGrid=data.frame(mtry=7))
predict_train_RF <- predict(mod_RF, train_final)
ConfM_train_RF <- confusionMatrix(predict_train_RF,as.factor(train_final$classe))
predict_test_RF <- predict(mod_RF, test_final)
ConfM_test_RF <- confusionMatrix(predict_test_RF,as.factor(test_final$classe))
print(ConfM_test_RF)
```
Random Forest model predicts with higher accuracy overall 0.9944 compared to Decision Tree Modeling Accuracy which was 0.4997.

Because Random Forest gives the best prediction,there is no need for cross-validation and define out of sample error using a seperate test set. 

Then I used random forest model to predict 20 different test cases. 
```{r}
print(predict(mod_RF, newdata=testing))
```
